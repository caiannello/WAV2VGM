## About AI Models

These are generated by the pytorch model trainer in 'src/pytorch_train_NN.py', which ingests the big training set data from 'training_sets/', and then outputs a model to 'models/torch_model.pth'. The model, if properly trained, should output a suitable synthesizer configuration when given an arbitrary frequency spectrum.

The training sets are generated using 'src/make_training_set.py', which takes several hours to generate a suitably huge set which contains two files: regs and spects. Regs are random synthesizer configurations, and spects are the frequency spectra obtained when using each configuration. 

WAV2VGM uses this model when doing the AI method of converting a WAV file to a VGM file. It should (hopefully) leverage the more advanced features of OPL3 (or other synthesizer) to give a more faithful, or at least a more interesting-sounding, renditions of the input file.



