# ------------------------------------------------------------------------
# WAV2VGM
#
# Trains a convolutional neural network how to (hopefully)...
# 
# Infer synthesizer config for a given frequency spectrum.
#
# The data model and training set getters are in model_definitions.py
#
# Training data is generated by make_training_set.py
# 
# ------------------------------------------------------------------------
import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from   torch.utils.data import DataLoader, random_split
import matplotlib.pyplot as plt
from   model_definitions import OPL3Model, OPL3Dataset
# ------------------------------------------------------------------------
# Report device configuration 

devname = "cuda" if torch.cuda.is_available() else "cpu"
print(f"\n\nCUDA or CPU?   I'm selecting: ---> {devname.upper()} <---")
device = torch.device(devname)

# get path to our own file

dir_path            = os.path.dirname(os.path.realpath(__file__))

# Parameters

spect_filepath      = dir_path+'/../training_sets/opl3_training_spects.bin'  # training test data
synthcfg_filepath   = dir_path+'/../training_sets/opl3_training_regs.bin'    # training truth truth
model_filepath      = dir_path+'/../models/torch_model.pth'                  # trained model output
batch_size = 32
epochs = 4        # 50   keep this low until we settle on the dataset and model
early_stopping_patience = 5

print('''
  TODO: Add code to sanity check training set before starting.
  In particular, make sure inputs and outputs stay in sync 
  throughout the file, since the generator can be stopped 
  and restarted. This could involve re-rendering some of synth
  configs including the last one, to make sure the spectra still
  match.
  ''')

# TODO: Allow resumption of training on a preexisting 
# model, but wipe it and start over if the model changed.

# Load dataset and split into training and validation sets
# (getter defined in model_definitions.py)

full_dataset = OPL3Dataset(spect_filepath, synthcfg_filepath)
dataset_size = len(full_dataset)
val_size = int(0.2 * dataset_size)
train_size = dataset_size - val_size
train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

model = OPL3Model().to(device)

# Loss and optimizer
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# ------------------------------------------------------------------------

# Training
train_losses, val_losses = [], []
best_val_loss = float('inf')
early_stopping_counter = 0

for epoch in range(epochs):
    model.train()
    running_loss = 0.0
    for batch_idx, (inputs, targets) in enumerate(train_loader, 1):
        inputs, targets = inputs.to(device), targets.to(device)

        # Forward pass
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        
        # Backward and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
        
        # Print status update every 10 batches
        if batch_idx % 10 == 0:
            print(f"Epoch [{epoch+1}/{epochs}], Batch [{batch_idx}/{len(train_loader)}], "
                  f"Batch Loss: {loss.item():.4f}")

    avg_train_loss = running_loss / len(train_loader)
    train_losses.append(avg_train_loss)

    # Validation
    model.eval()
    running_val_loss = 0.0
    with torch.no_grad():
        for inputs, targets in val_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            val_loss = criterion(outputs, targets)
            running_val_loss += val_loss.item()
    
    avg_val_loss = running_val_loss / len(val_loader)
    val_losses.append(avg_val_loss)

    print(f"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}")

    # Early Stopping
    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        torch.save(model.state_dict(), model_filepath)
        early_stopping_counter = 0
    else:
        early_stopping_counter += 1
        if early_stopping_counter >= early_stopping_patience:
            print("Early stopping triggered.")
            break

# Load the best model
model.load_state_dict(torch.load(model_filepath))

# sanity Check
print("Sanity check:")
val_loader_iter = iter(val_loader)
for sample_input, true_output in val_loader_iter:
    sample_input, true_output = sample_input.to(device), true_output.to(device)
    predicted_output = model(sample_input)
    print("Sample Input:", sample_input.cpu().numpy())
    print("True Output:", true_output.cpu().numpy())
    print("Predicted Output:", predicted_output.detach().numpy())
    break

# plotting the Loss graph

plt.plot(train_losses, label='Training Loss')
plt.plot(val_losses, label='Validation Loss')
plt.legend()
plt.show()

print('Program END.')
# ------------------------------------------------------------------------
# ------------------------------------------------------------------------
